{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Word Frequency Analysis: Rising and Declining Terms in Singapore Budget Speeches\n\nThis notebook analyzes word frequency trends across **67 Singapore budget speeches** spanning from 1965 to 2026 (61 unique years, with some years containing multiple supplementary budgets).\n\n## Methodology\n\n### Objective\nIdentify the **top 12 rising words** and **top 12 declining words** by comparing:\n- **First 10 years**: 1965-1975 (speeches from the early era)\n- **Last 10 years**: 2017-2026 (speeches from the recent era)\n\n### Key Metric: Normalized Frequency\nTo compare word usage across speeches of different lengths, we use **mentions per 10,000 words**:\n\n$$\\text{Normalized Frequency} = \\frac{\\text{Word Count}}{\\text{Total Words in Document}} \\times 10,000$$\n\n### Rising/Declining Score\nFor each word, we calculate the **change in average normalized frequency**:\n\n$$\\text{Change Score} = \\bar{f}_{\\text{last 10 years}} - \\bar{f}_{\\text{first 10 years}}$$\n\nWhere $\\bar{f}$ is the mean normalized frequency across the period.\n\n- **Positive scores** indicate **rising** words (more frequent in recent years)\n- **Negative scores** indicate **declining** words (less frequent in recent years)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process the Corpus\n",
    "\n",
    "Each file is named with the pattern: `YYYY-MM-DD_Minister_Name.txt`\n",
    "\n",
    "We extract the year from the filename and aggregate speeches by year (some years have multiple supplementary budgets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Path to corpus\nCORPUS_PATH = Path('/Users/wongpeiting/Desktop/CU/python-work/budget-strict/corpus')\n\ndef extract_year(filename):\n    \"\"\"Extract the year from filename like '1965-12-13_Lim_Kim_San.txt'\"\"\"\n    match = re.match(r'(\\d{4})', filename)\n    return int(match.group(1)) if match else None\n\n# UK to US spelling mappings (normalize to US spelling)\nUK_TO_US_SPELLING = {\n    # -ise -> -ize\n    'organisation': 'organization', 'organisations': 'organizations',\n    'organise': 'organize', 'organised': 'organized', 'organising': 'organizing',\n    'recognise': 'recognize', 'recognised': 'recognized', 'recognising': 'recognizing',\n    'realise': 'realize', 'realised': 'realized', 'realising': 'realizing',\n    'utilise': 'utilize', 'utilised': 'utilized', 'utilising': 'utilizing',\n    'maximise': 'maximize', 'maximised': 'maximized', 'maximising': 'maximizing',\n    'minimise': 'minimize', 'minimised': 'minimized', 'minimising': 'minimizing',\n    'prioritise': 'prioritize', 'prioritised': 'prioritized', 'prioritising': 'prioritizing',\n    'emphasise': 'emphasize', 'emphasised': 'emphasized', 'emphasising': 'emphasizing',\n    'stabilise': 'stabilize', 'stabilised': 'stabilized', 'stabilising': 'stabilizing',\n    'modernise': 'modernize', 'modernised': 'modernized', 'modernising': 'modernizing',\n    'liberalise': 'liberalize', 'liberalised': 'liberalized', 'liberalising': 'liberalizing',\n    'privatise': 'privatize', 'privatised': 'privatized', 'privatising': 'privatizing',\n    'specialise': 'specialize', 'specialised': 'specialized', 'specialising': 'specializing',\n    'subsidise': 'subsidize', 'subsidised': 'subsidized', 'subsidising': 'subsidizing',\n    'computerise': 'computerize', 'computerised': 'computerized', 'computerising': 'computerizing',\n    'standardise': 'standardize', 'standardised': 'standardized', 'standardising': 'standardizing',\n    'harmonise': 'harmonize', 'harmonised': 'harmonized', 'harmonising': 'harmonizing',\n    'capitalise': 'capitalize', 'capitalised': 'capitalized', 'capitalising': 'capitalizing',\n    'centralise': 'centralize', 'centralised': 'centralized', 'centralising': 'centralizing',\n    'decentralise': 'decentralize', 'decentralised': 'decentralized', 'decentralising': 'decentralizing',\n    \n    # -our -> -or\n    'labour': 'labor', 'labours': 'labors',\n    'colour': 'color', 'colours': 'colors', 'coloured': 'colored',\n    'favour': 'favor', 'favours': 'favors', 'favoured': 'favored', 'favourable': 'favorable',\n    'honour': 'honor', 'honours': 'honors', 'honoured': 'honored', 'honourable': 'honorable',\n    'neighbour': 'neighbor', 'neighbours': 'neighbors', 'neighbourhood': 'neighborhood',\n    'behaviour': 'behavior', 'behaviours': 'behaviors',\n    'endeavour': 'endeavor', 'endeavours': 'endeavors',\n    \n    # -re -> -er\n    'centre': 'center', 'centres': 'centers', 'centred': 'centered',\n    'metre': 'meter', 'metres': 'meters',\n    'litre': 'liter', 'litres': 'liters',\n    'fibre': 'fiber', 'fibres': 'fibers',\n    'theatre': 'theater', 'theatres': 'theaters',\n    \n    # -ogue -> -og\n    'catalogue': 'catalog', 'catalogues': 'catalogs',\n    'dialogue': 'dialog', 'dialogues': 'dialogs',\n    'analogue': 'analog',\n    \n    # -ence -> -ense\n    'defence': 'defense', 'defences': 'defenses',\n    'offence': 'offense', 'offences': 'offenses',\n    'licence': 'license', 'licences': 'licenses',\n    \n    # -gramme -> -gram\n    'programme': 'program', 'programmes': 'programs', 'programmed': 'programmed',\n    'kilogramme': 'kilogram', 'kilogrammes': 'kilograms',\n    \n    # Other common variations\n    'cheque': 'check', 'cheques': 'checks',\n    'grey': 'gray',\n    'ageing': 'aging',\n    'judgement': 'judgment', 'judgements': 'judgments',\n    'acknowledgement': 'acknowledgment', 'acknowledgements': 'acknowledgments',\n    'fulfil': 'fulfill', 'fulfilled': 'fulfilled', 'fulfilling': 'fulfilling', 'fulfilment': 'fulfillment',\n    'enrol': 'enroll', 'enrolled': 'enrolled', 'enrolment': 'enrollment',\n    'skilful': 'skillful',\n    'instalment': 'installment', 'instalments': 'installments',\n    'counselling': 'counseling', 'counsellor': 'counselor',\n    'travelling': 'traveling', 'traveller': 'traveler',\n    'modelling': 'modeling',\n    'levelling': 'leveling',\n    'labelling': 'labeling',\n    'signalling': 'signaling',\n    'cancelled': 'canceled', 'cancelling': 'canceling',\n}\n\ndef normalize_spelling(word):\n    \"\"\"Normalize UK spelling to US spelling.\"\"\"\n    return UK_TO_US_SPELLING.get(word, word)\n\ndef tokenize(text):\n    \"\"\"Convert text to lowercase words, normalize spelling, keeping only alphabetic tokens.\"\"\"\n    # Remove numbers and punctuation, convert to lowercase\n    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n    # Normalize UK -> US spelling\n    words = [normalize_spelling(w) for w in words]\n    return words\n\n# Load all speeches grouped by year\nspeeches_by_year = defaultdict(list)\nfile_count = 0\n\nfor filepath in sorted(CORPUS_PATH.glob('*.txt')):\n    year = extract_year(filepath.name)\n    if year:\n        file_count += 1\n        with open(filepath, 'r', encoding='utf-8') as f:\n            text = f.read()\n        speeches_by_year[year].append(text)\n\nprint(f\"Total speeches loaded: {file_count}\")\nprint(f\"Unique years: {len(speeches_by_year)}\")\nprint(f\"Year range: {min(speeches_by_year.keys())} - {max(speeches_by_year.keys())}\")\nprint(f\"\\nUK/US spelling variants normalized: {len(UK_TO_US_SPELLING)}\")\n\n# Show years with multiple speeches\nprint(\"\\nYears with multiple speeches:\")\nfor year in sorted(speeches_by_year.keys()):\n    if len(speeches_by_year[year]) > 1:\n        print(f\"  {year}: {len(speeches_by_year[year])} speeches\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Word Frequencies Per Year\n",
    "\n",
    "For each year, we:\n",
    "1. Concatenate all speeches for that year\n",
    "2. Tokenize into words\n",
    "3. Count word occurrences\n",
    "4. Calculate total word count for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words per year\n",
    "word_counts_by_year = {}  # year -> Counter of words\n",
    "total_words_by_year = {}  # year -> total word count\n",
    "\n",
    "for year, texts in sorted(speeches_by_year.items()):\n",
    "    combined_text = ' '.join(texts)\n",
    "    words = tokenize(combined_text)\n",
    "    word_counts_by_year[year] = Counter(words)\n",
    "    total_words_by_year[year] = len(words)\n",
    "\n",
    "# Display summary\n",
    "print(\"Total words per year:\\n\")\n",
    "for year in sorted(total_words_by_year.keys()):\n",
    "    print(f\"{year}: {total_words_by_year[year]:,} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Normalized Frequency Matrix\n",
    "\n",
    "Create a DataFrame where:\n",
    "- **Rows** = unique words\n",
    "- **Columns** = years\n",
    "- **Values** = normalized frequency (mentions per 10,000 words)\n",
    "\n",
    "$$f_{w,y} = \\frac{\\text{count}(w, y)}{\\text{total}(y)} \\times 10,000$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique words across all years\n",
    "all_words = set()\n",
    "for counter in word_counts_by_year.values():\n",
    "    all_words.update(counter.keys())\n",
    "\n",
    "print(f\"Total unique words: {len(all_words):,}\")\n",
    "\n",
    "# Build normalized frequency matrix\n",
    "years = sorted(word_counts_by_year.keys())\n",
    "freq_data = {}\n",
    "\n",
    "for year in years:\n",
    "    total = total_words_by_year[year]\n",
    "    counter = word_counts_by_year[year]\n",
    "    # Normalized frequency: mentions per 10,000 words\n",
    "    freq_data[year] = {word: (counter.get(word, 0) / total) * 10000 for word in all_words}\n",
    "\n",
    "# Create DataFrame\n",
    "freq_df = pd.DataFrame(freq_data)\n",
    "freq_df = freq_df.fillna(0)\n",
    "\n",
    "print(f\"\\nFrequency matrix shape: {freq_df.shape} (words x years)\")\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Early and Recent Periods\n",
    "\n",
    "We compare:\n",
    "- **Early period**: First 10 distinct years in the corpus\n",
    "- **Recent period**: Last 10 distinct years in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define periods\n",
    "all_years = sorted(freq_df.columns)\n",
    "early_years = all_years[:10]\n",
    "recent_years = all_years[-10:]\n",
    "\n",
    "print(f\"Early period years: {early_years}\")\n",
    "print(f\"Recent period years: {recent_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Rising and Declining Words\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "For each word $w$:\n",
    "\n",
    "$$\\bar{f}_{\\text{early}}(w) = \\frac{1}{|Y_{\\text{early}}|} \\sum_{y \\in Y_{\\text{early}}} f_{w,y}$$\n",
    "\n",
    "$$\\bar{f}_{\\text{recent}}(w) = \\frac{1}{|Y_{\\text{recent}}|} \\sum_{y \\in Y_{\\text{recent}}} f_{w,y}$$\n",
    "\n",
    "$$\\Delta(w) = \\bar{f}_{\\text{recent}}(w) - \\bar{f}_{\\text{early}}(w)$$\n",
    "\n",
    "- **Rising words**: Words with highest positive $\\Delta$\n",
    "- **Declining words**: Words with lowest (most negative) $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean frequency for each period\n",
    "early_mean = freq_df[early_years].mean(axis=1)\n",
    "recent_mean = freq_df[recent_years].mean(axis=1)\n",
    "\n",
    "# Calculate change score\n",
    "change_score = recent_mean - early_mean\n",
    "\n",
    "# Create summary DataFrame\n",
    "word_analysis = pd.DataFrame({\n",
    "    'word': freq_df.index,\n",
    "    'early_mean_freq': early_mean.values,\n",
    "    'recent_mean_freq': recent_mean.values,\n",
    "    'change_score': change_score.values\n",
    "})\n",
    "\n",
    "# Filter out very rare words (must appear at least 5 times per 10k words on average in EITHER period)\n",
    "# This filters out noise from very infrequent words\n",
    "MIN_FREQ_THRESHOLD = 1.0  # At least 1 mention per 10,000 words on average\n",
    "word_analysis_filtered = word_analysis[\n",
    "    (word_analysis['early_mean_freq'] >= MIN_FREQ_THRESHOLD) | \n",
    "    (word_analysis['recent_mean_freq'] >= MIN_FREQ_THRESHOLD)\n",
    "]\n",
    "\n",
    "print(f\"Words meeting minimum frequency threshold: {len(word_analysis_filtered):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Expanded stopwords list - common words that don't carry meaning\n# Includes: articles, prepositions, pronouns, auxiliaries, conjunctions, \n# common verbs, adverbs, and budget-speech-specific filler words\n\nSTOPWORDS = {\n    # Articles & determiners\n    'the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her',\n    'its', 'our', 'their', 'some', 'any', 'no', 'every', 'each', 'all', 'both',\n    'few', 'many', 'much', 'most', 'other', 'another', 'such', 'what', 'which',\n    'whose', 'whatever', 'whichever',\n    \n    # Pronouns\n    'i', 'me', 'we', 'us', 'you', 'he', 'him', 'she', 'her', 'it', 'they', 'them',\n    'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'themselves',\n    'who', 'whom', 'whose', 'whoever', 'whomever',\n    \n    # Prepositions\n    'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'into', 'through',\n    'during', 'before', 'after', 'above', 'below', 'between', 'under', 'over', 'out',\n    'up', 'down', 'off', 'about', 'against', 'among', 'around', 'behind', 'beside',\n    'beyond', 'within', 'without', 'along', 'across', 'upon', 'towards', 'toward',\n    'throughout', 'despite', 'via', 'per', 'including', 'regarding', 'concerning',\n    \n    # Conjunctions\n    'and', 'or', 'but', 'nor', 'so', 'yet', 'for', 'because', 'since', 'although',\n    'though', 'while', 'whereas', 'if', 'unless', 'until', 'when', 'whenever',\n    'where', 'wherever', 'whether', 'however', 'therefore', 'thus', 'hence',\n    'moreover', 'furthermore', 'nevertheless', 'nonetheless', 'otherwise',\n    \n    # Auxiliary/Modal verbs\n    'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being',\n    'have', 'has', 'had', 'having',\n    'do', 'does', 'did', 'doing', 'done',\n    'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'can', 'could',\n    'need', 'dare', 'ought', 'used',\n    \n    # Common verbs (too generic to be meaningful)\n    'get', 'got', 'getting', 'gets',\n    'make', 'made', 'making', 'makes',\n    'take', 'took', 'taken', 'taking', 'takes',\n    'give', 'gave', 'given', 'giving', 'gives',\n    'go', 'went', 'gone', 'going', 'goes',\n    'come', 'came', 'coming', 'comes',\n    'see', 'saw', 'seen', 'seeing', 'sees',\n    'know', 'knew', 'known', 'knowing', 'knows',\n    'think', 'thought', 'thinking', 'thinks',\n    'say', 'said', 'saying', 'says',\n    'tell', 'told', 'telling', 'tells',\n    'put', 'putting', 'puts',\n    'let', 'letting', 'lets',\n    'keep', 'kept', 'keeping', 'keeps',\n    'set', 'setting', 'sets',\n    'seem', 'seemed', 'seeming', 'seems',\n    'want', 'wanted', 'wanting', 'wants',\n    'look', 'looked', 'looking', 'looks',\n    'use', 'used', 'using', 'uses',\n    'find', 'found', 'finding', 'finds',\n    'show', 'showed', 'shown', 'showing', 'shows',\n    'try', 'tried', 'trying', 'tries',\n    'leave', 'left', 'leaving', 'leaves',\n    'call', 'called', 'calling', 'calls',\n    'ask', 'asked', 'asking', 'asks',\n    'turn', 'turned', 'turning', 'turns',\n    'begin', 'began', 'begun', 'beginning', 'begins',\n    'start', 'started', 'starting', 'starts',\n    'move', 'moved', 'moving', 'moves',\n    'run', 'ran', 'running', 'runs',\n    'bring', 'brought', 'bringing', 'brings',\n    'hold', 'held', 'holding', 'holds',\n    'write', 'wrote', 'written', 'writing', 'writes',\n    'read', 'reading', 'reads',\n    'learn', 'learned', 'learnt', 'learning', 'learns',\n    'change', 'changed', 'changing', 'changes',\n    'follow', 'followed', 'following', 'follows',\n    'stop', 'stopped', 'stopping', 'stops',\n    'mean', 'meant', 'meaning', 'means',\n    'add', 'added', 'adding', 'adds',\n    'play', 'played', 'playing', 'plays',\n    'pay', 'paid', 'paying', 'pays',\n    'hear', 'heard', 'hearing', 'hears',\n    'include', 'included', 'including', 'includes',\n    'believe', 'believed', 'believing', 'believes',\n    'allow', 'allowed', 'allowing', 'allows',\n    'meet', 'met', 'meeting', 'meets',\n    'lead', 'led', 'leading', 'leads',\n    'live', 'lived', 'living', 'lives',\n    'stand', 'stood', 'standing', 'stands',\n    'happen', 'happened', 'happening', 'happens',\n    'carry', 'carried', 'carrying', 'carries',\n    'talk', 'talked', 'talking', 'talks',\n    'appear', 'appeared', 'appearing', 'appears',\n    'produce', 'produced', 'producing', 'produces',\n    'sit', 'sat', 'sitting', 'sits',\n    'offer', 'offered', 'offering', 'offers',\n    'consider', 'considered', 'considering', 'considers',\n    'expect', 'expected', 'expecting', 'expects',\n    'suggest', 'suggested', 'suggesting', 'suggests',\n    'remain', 'remained', 'remaining', 'remains',\n    'require', 'required', 'requiring', 'requires',\n    'report', 'reported', 'reporting', 'reports',\n    'decide', 'decided', 'deciding', 'decides',\n    'reach', 'reached', 'reaching', 'reaches',\n    'rise', 'rose', 'risen', 'rising', 'rises',\n    'pass', 'passed', 'passing', 'passes',\n    'sell', 'sold', 'selling', 'sells',\n    'buy', 'bought', 'buying', 'buys',\n    'create', 'created', 'creating', 'creates',\n    'spend', 'spent', 'spending', 'spends',\n    'grow', 'grew', 'grown', 'growing', 'grows',\n    'open', 'opened', 'opening', 'opens',\n    'walk', 'walked', 'walking', 'walks',\n    'win', 'won', 'winning', 'wins',\n    'lose', 'lost', 'losing', 'loses',\n    'send', 'sent', 'sending', 'sends',\n    'build', 'built', 'building', 'builds',\n    'fall', 'fell', 'fallen', 'falling', 'falls',\n    'cut', 'cutting', 'cuts',\n    'kill', 'killed', 'killing', 'kills',\n    'reduce', 'reduced', 'reducing', 'reduces',\n    'develop', 'developed', 'developing', 'develops',\n    'remember', 'remembered', 'remembering', 'remembers',\n    'speak', 'spoke', 'spoken', 'speaking', 'speaks',\n    'agree', 'agreed', 'agreeing', 'agrees',\n    'raise', 'raised', 'raising', 'raises',\n    'pick', 'picked', 'picking', 'picks',\n    'pull', 'pulled', 'pulling', 'pulls',\n    'push', 'pushed', 'pushing', 'pushes',\n    'watch', 'watched', 'watching', 'watches',\n    'drive', 'drove', 'driven', 'driving', 'drives',\n    'break', 'broke', 'broken', 'breaking', 'breaks',\n    'draw', 'drew', 'drawn', 'drawing', 'draws',\n    'explain', 'explained', 'explaining', 'explains',\n    'receive', 'received', 'receiving', 'receives',\n    'determine', 'determined', 'determining', 'determines',\n    'serve', 'served', 'serving', 'serves',\n    'apply', 'applied', 'applying', 'applies',\n    'prepare', 'prepared', 'preparing', 'prepares',\n    'accept', 'accepted', 'accepting', 'accepts',\n    'achieve', 'achieved', 'achieving', 'achieves',\n    'obtain', 'obtained', 'obtaining', 'obtains',\n    'contain', 'contained', 'containing', 'contains',\n    'present', 'presented', 'presenting', 'presents',\n    'exist', 'existed', 'existing', 'exists',\n    'result', 'resulted', 'resulting', 'results',\n    'continue', 'continued', 'continuing', 'continues',\n    'provide', 'provided', 'providing', 'provides',\n    'ensure', 'ensured', 'ensuring', 'ensures',\n    'enable', 'enabled', 'enabling', 'enables',\n    'increase', 'increased', 'increasing', 'increases',\n    'decrease', 'decreased', 'decreasing', 'decreases',\n    'expand', 'expanded', 'expanding', 'expands',\n    'extend', 'extended', 'extending', 'extends',\n    'maintain', 'maintained', 'maintaining', 'maintains',\n    'establish', 'established', 'establishing', 'establishes',\n    'address', 'addressed', 'addressing', 'addresses',\n    'implement', 'implemented', 'implementing', 'implements',\n    'enhance', 'enhanced', 'enhancing', 'enhances',\n    'strengthen', 'strengthened', 'strengthening', 'strengthens',\n    'improve', 'improved', 'improving', 'improves',\n    'invest', 'invested', 'investing', 'invests',\n    'fund', 'funded', 'funding', 'funds',\n    'allocate', 'allocated', 'allocating', 'allocates',\n    \n    # Adverbs\n    'also', 'just', 'only', 'very', 'even', 'well', 'back', 'still', 'too',\n    'here', 'there', 'now', 'then', 'again', 'already', 'always', 'never',\n    'often', 'sometimes', 'usually', 'really', 'quite', 'rather', 'almost',\n    'enough', 'especially', 'particularly', 'certainly', 'clearly', 'simply',\n    'finally', 'actually', 'recently', 'probably', 'perhaps', 'maybe',\n    'indeed', 'currently', 'recently', 'generally', 'specifically', 'directly',\n    'certainly', 'obviously', 'definitely', 'necessarily', 'relatively',\n    'eventually', 'immediately', 'effectively', 'significantly', 'substantially',\n    \n    # Numbers/quantity words & numerical terms (budget filler)\n    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n    'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen',\n    'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty', 'sixty',\n    'seventy', 'eighty', 'ninety', 'hundred', 'thousand',\n    'million', 'millions', 'billion', 'billions', 'trillion', 'trillions',\n    'first', 'second', 'third', 'fourth', 'fifth', 'last', 'next', 'previous',\n    'same', 'different', 'various', 'several', 'whole', 'entire', 'full',\n    'half', 'part', 'less', 'least', 'more', 'most', 'further', 'additional',\n    'total', 'totals', 'totalling', 'overall', 'aggregate', 'sum', 'average',\n    'approximately', 'roughly', 'nearly', 'almost', 'around', 'circa', 'about',\n    \n    # Common adjectives (too generic)\n    'good', 'better', 'best', 'bad', 'worse', 'worst',\n    'great', 'small', 'large', 'big', 'little', 'long', 'short',\n    'high', 'low', 'higher', 'lower', 'highest', 'lowest',\n    'new', 'old', 'young', 'early', 'late',\n    'important', 'major', 'main', 'key', 'significant', 'able', 'certain',\n    'clear', 'likely', 'possible', 'available', 'necessary', 'true', 'real',\n    'right', 'wrong', 'sure', 'hard', 'easy', 'simple', 'complex',\n    'own', 'particular', 'special', 'specific', 'general', 'common',\n    'similar', 'basic', 'free', 'full', 'single', 'open', 'close',\n    'strong', 'weak', 'positive', 'negative', 'public', 'private',\n    \n    # Budget speech fillers & parliamentary language\n    'mr', 'mrs', 'ms', 'sir', 'madam', 'speaker', 'chairman', 'member', 'members',\n    'honorable', 'honourable', 'minister', 'ministers', 'government', 'parliament',\n    'singapore', 'singaporean', 'singaporeans', 'thank', 'please', 'like', 'way', 'ways',\n    'thing', 'things', 'time', 'times', 'year', 'years', 'month', 'months', 'day', 'days',\n    'week', 'weeks', 'quarter', 'quarters', 'annual', 'annually', 'fiscal',\n    'point', 'points', 'fact', 'facts', 'case', 'cases', 'example', 'examples',\n    'number', 'numbers', 'amount', 'amounts', 'level', 'levels', 'rate', 'rates',\n    'term', 'terms', 'area', 'areas', 'part', 'parts', 'place', 'places',\n    'end', 'ends', 'side', 'sides', 'kind', 'kinds', 'sort', 'sorts', 'type', 'types',\n    'form', 'forms', 'group', 'groups', 'line', 'lines', 'order', 'orders',\n    'problem', 'problems', 'question', 'questions', 'issue', 'issues',\n    'reason', 'reasons', 'result', 'results', 'effect', 'effects',\n    'need', 'needs', 'view', 'views', 'idea', 'ideas', 'interest', 'interests',\n    'system', 'systems', 'plan', 'plans', 'period', 'periods', 'state', 'states',\n    'matter', 'matters', 'basis', 'base', 'range', 'ranges',\n    'cent', 'cents', 'percent', 'percentage', 'percentages', 'proportion', 'proportions',\n    'figure', 'figures', 'estimate', 'estimates', 'estimated', 'projection', 'projections',\n    'budget', 'budgets', 'budgeted', 'budgeting', 'expenditure', 'expenditures',\n    'revenue', 'revenues', 'income', 'incomes', 'spending', 'spendings',\n    'growth', 'gdp', 'economy', 'economic', 'economies', 'financial', 'finance',\n    'policy', 'policies', 'measure', 'measures', 'initiative', 'initiatives',\n    'program', 'programs', 'scheme', 'schemes', 'project', 'projects',\n    'sector', 'sectors', 'industry', 'industries', 'industrial',\n    \n    # Single letters and short tokens\n    's', 't', 'd', 'll', 've', 're', 'm', 'don', 'doesn', 'didn', 'won', 'wouldn',\n    'couldn', 'shouldn', 'isn', 'aren', 'wasn', 'weren', 'hasn', 'haven', 'hadn',\n    'fy', 'eg', 'ie', 'etc', 'vs',\n}\n\n# Filter out stopwords\nword_analysis_clean = word_analysis_filtered[~word_analysis_filtered['word'].isin(STOPWORDS)]\n\nprint(f\"Stopwords defined: {len(STOPWORDS)}\")\nprint(f\"Words after removing stopwords: {len(word_analysis_clean):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 12 rising and declining words\n",
    "top_rising = word_analysis_clean.nlargest(12, 'change_score')\n",
    "top_declining = word_analysis_clean.nsmallest(12, 'change_score')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP 12 RISING WORDS (most increased in recent years)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Word':<15} {'Early Freq':>12} {'Recent Freq':>12} {'Change':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for _, row in top_rising.iterrows():\n",
    "    print(f\"{row['word']:<15} {row['early_mean_freq']:>12.2f} {row['recent_mean_freq']:>12.2f} {row['change_score']:>+10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP 12 DECLINING WORDS (most decreased in recent years)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Word':<15} {'Early Freq':>12} {'Recent Freq':>12} {'Change':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for _, row in top_declining.iterrows():\n",
    "    print(f\"{row['word']:<15} {row['early_mean_freq']:>12.2f} {row['recent_mean_freq']:>12.2f} {row['change_score']:>+10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Yearly Frequency Data for Selected Words\n",
    "\n",
    "Now we prepare the full time series data for each of our 24 selected words (12 rising + 12 declining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected words\n",
    "rising_words = top_rising['word'].tolist()\n",
    "declining_words = top_declining['word'].tolist()\n",
    "selected_words = rising_words + declining_words\n",
    "\n",
    "print(f\"Rising words: {rising_words}\")\n",
    "print(f\"\\nDeclining words: {declining_words}\")\n",
    "\n",
    "# Extract time series for selected words\n",
    "selected_freq_df = freq_df.loc[selected_words].copy()\n",
    "selected_freq_df.index.name = 'word'\n",
    "\n",
    "# Transpose for easier plotting (years as rows, words as columns)\n",
    "selected_freq_transposed = selected_freq_df.T\n",
    "selected_freq_transposed.index.name = 'year'\n",
    "\n",
    "print(f\"\\nTime series shape: {selected_freq_transposed.shape}\")\n",
    "selected_freq_transposed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Rising Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rising words\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, word in enumerate(rising_words):\n",
    "    ax = axes[i]\n",
    "    years_plot = selected_freq_transposed.index\n",
    "    values = selected_freq_transposed[word]\n",
    "    \n",
    "    ax.fill_between(years_plot, values, alpha=0.3, color='#2ecc71')\n",
    "    ax.plot(years_plot, values, color='#27ae60', linewidth=2)\n",
    "    ax.set_title(f'\"{word}\"', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('per 10k words')\n",
    "    ax.set_xlim(years_plot.min(), years_plot.max())\n",
    "    ax.set_ylim(0, None)\n",
    "    \n",
    "    # Add trend annotation\n",
    "    early_avg = selected_freq_transposed.loc[early_years, word].mean()\n",
    "    recent_avg = selected_freq_transposed.loc[recent_years, word].mean()\n",
    "    change = recent_avg - early_avg\n",
    "    ax.annotate(f'+{change:.1f}', xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "                fontsize=12, ha='right', va='top', color='#27ae60', fontweight='bold')\n",
    "\n",
    "plt.suptitle('TOP 12 RISING WORDS\\n(mentions per 10,000 words)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rising_words_sparklines.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Declining Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot declining words\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, word in enumerate(declining_words):\n",
    "    ax = axes[i]\n",
    "    years_plot = selected_freq_transposed.index\n",
    "    values = selected_freq_transposed[word]\n",
    "    \n",
    "    ax.fill_between(years_plot, values, alpha=0.3, color='#e74c3c')\n",
    "    ax.plot(years_plot, values, color='#c0392b', linewidth=2)\n",
    "    ax.set_title(f'\"{word}\"', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('per 10k words')\n",
    "    ax.set_xlim(years_plot.min(), years_plot.max())\n",
    "    ax.set_ylim(0, None)\n",
    "    \n",
    "    # Add trend annotation\n",
    "    early_avg = selected_freq_transposed.loc[early_years, word].mean()\n",
    "    recent_avg = selected_freq_transposed.loc[recent_years, word].mean()\n",
    "    change = recent_avg - early_avg\n",
    "    ax.annotate(f'{change:.1f}', xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "                fontsize=12, ha='right', va='top', color='#c0392b', fontweight='bold')\n",
    "\n",
    "plt.suptitle('TOP 12 DECLINING WORDS\\n(mentions per 10,000 words)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('declining_words_sparklines.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Data for D3 Visualization\n",
    "\n",
    "Export the time series data in a format suitable for D3 sparklines on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Prepare data for export\nexport_data = {\n    'metadata': {\n        'total_speeches': 67,\n        'unique_years': len(list(speeches_by_year.keys())),\n        'year_range': [min(speeches_by_year.keys()), max(speeches_by_year.keys())],\n        'early_period': list(early_years),\n        'recent_period': list(recent_years),\n        'metric': 'mentions per 10,000 words'\n    },\n    'rising_words': [],\n    'declining_words': []\n}\n\nfor word in rising_words:\n    word_data = {\n        'word': word,\n        'trend': 'rising',\n        'early_avg': round(selected_freq_transposed.loc[early_years, word].mean(), 2),\n        'recent_avg': round(selected_freq_transposed.loc[recent_years, word].mean(), 2),\n        'change': round(selected_freq_transposed.loc[recent_years, word].mean() - \n                       selected_freq_transposed.loc[early_years, word].mean(), 2),\n        'yearly_data': [\n            {'year': int(year), 'frequency': round(selected_freq_transposed.loc[year, word], 2)}\n            for year in selected_freq_transposed.index\n        ]\n    }\n    export_data['rising_words'].append(word_data)\n\nfor word in declining_words:\n    word_data = {\n        'word': word,\n        'trend': 'declining',\n        'early_avg': round(selected_freq_transposed.loc[early_years, word].mean(), 2),\n        'recent_avg': round(selected_freq_transposed.loc[recent_years, word].mean(), 2),\n        'change': round(selected_freq_transposed.loc[recent_years, word].mean() - \n                       selected_freq_transposed.loc[early_years, word].mean(), 2),\n        'yearly_data': [\n            {'year': int(year), 'frequency': round(selected_freq_transposed.loc[year, word], 2)}\n            for year in selected_freq_transposed.index\n        ]\n    }\n    export_data['declining_words'].append(word_data)\n\n# Save to JSON\nwith open('word_frequency_data.json', 'w') as f:\n    json.dump(export_data, f, indent=2)\n\nprint(\"Data exported to word_frequency_data.json\")\nprint(f\"\\nMetadata:\")\nprint(json.dumps(export_data['metadata'], indent=2))\nprint(f\"\\nSample rising word entry:\")\nprint(json.dumps(export_data['rising_words'][0], indent=2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Word': selected_words,\n",
    "    'Trend': ['Rising']*12 + ['Declining']*12,\n",
    "    'Early Avg (per 10k)': [selected_freq_transposed.loc[early_years, w].mean() for w in selected_words],\n",
    "    'Recent Avg (per 10k)': [selected_freq_transposed.loc[recent_years, w].mean() for w in selected_words],\n",
    "    'Change': [selected_freq_transposed.loc[recent_years, w].mean() - \n",
    "               selected_freq_transposed.loc[early_years, w].mean() for w in selected_words]\n",
    "})\n",
    "\n",
    "summary_df = summary_df.round(2)\n",
    "print(\"Summary of Word Frequency Changes\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "summary_df.to_csv('word_frequency_summary.csv', index=False)\n",
    "print(\"Summary exported to word_frequency_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Finance Minister \"Pet Words\" Analysis\n\nEach Finance Minister has a distinctive vocabulary that reflects their era and priorities. We identify **distinctive words** for each FM — words they used significantly more frequently than other ministers.\n\n### Methodology\n\nFor each Finance Minister $m$:\n\n1. Calculate the **normalized frequency** of each word across all their speeches:\n$$f_m(w) = \\frac{\\sum_{s \\in S_m} \\text{count}(w, s)}{\\sum_{s \\in S_m} \\text{total}(s)} \\times 10,000$$\n\n2. Calculate the **distinctiveness ratio** — how much more this FM used the word compared to other FMs:\n$$\\text{ratio}(w, m) = \\frac{f_m(w)}{\\max_{m' \\neq m} f_{m'}(w) + 0.5}$$\n\nThe $+0.5$ smoothing prevents division by zero and reduces noise from rarely-used words.\n\n3. Select words where:\n   - FM frequency ≥ 2.0 per 10k words (meaningful usage)\n   - Word length ≥ 5 characters (filter acronyms)\n   - Word is not in stopwords list\n   - Distinctiveness ratio is highest among valid words",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load speeches grouped by Finance Minister\ndef extract_fm(filename):\n    \"\"\"Extract FM name from filename like '1965-12-13_Lim_Kim_San.txt'\"\"\"\n    match = re.match(r'\\d{4}-\\d{2}-\\d{2}_(.+?)(?:_supplementary)?\\.txt', filename)\n    if match:\n        return match.group(1).replace('_', ' ')\n    return None\n\nspeeches_by_fm = defaultdict(list)\nfm_file_count = defaultdict(int)\n\nfor filepath in sorted(CORPUS_PATH.glob('*.txt')):\n    fm = extract_fm(filepath.name)\n    if fm:\n        fm_file_count[fm] += 1\n        with open(filepath, 'r', encoding='utf-8') as f:\n            text = f.read()\n        speeches_by_fm[fm].append(text)\n\nprint(\"Finance Ministers and their speech counts:\")\nprint(\"-\" * 40)\nfor fm in sorted(speeches_by_fm.keys(), key=lambda x: min([extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == x])):\n    years = [extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == fm]\n    print(f\"{fm}: {fm_file_count[fm]} speeches ({min(years)}-{max(years)})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate normalized word frequencies per FM\nword_counts_by_fm = {}  # fm -> Counter of words\ntotal_words_by_fm = {}  # fm -> total word count\n\nfor fm, texts in speeches_by_fm.items():\n    combined_text = ' '.join(texts)\n    words = tokenize(combined_text)\n    word_counts_by_fm[fm] = Counter(words)\n    total_words_by_fm[fm] = len(words)\n\n# Get all unique words across all FMs\nall_words_fm = set()\nfor counter in word_counts_by_fm.values():\n    all_words_fm.update(counter.keys())\n\n# Build normalized frequency matrix (words x FMs)\nfm_list = sorted(word_counts_by_fm.keys())\nfm_freq_data = {}\n\nfor fm in fm_list:\n    total = total_words_by_fm[fm]\n    counter = word_counts_by_fm[fm]\n    fm_freq_data[fm] = {word: (counter.get(word, 0) / total) * 10000 for word in all_words_fm}\n\nfm_freq_df = pd.DataFrame(fm_freq_data)\nfm_freq_df = fm_freq_df.fillna(0)\n\nprint(f\"FM frequency matrix shape: {fm_freq_df.shape} (words x FMs)\")\nprint(f\"\\nTotal words per FM:\")\nfor fm in fm_list:\n    print(f\"  {fm}: {total_words_by_fm[fm]:,} words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extended stopwords for FM analysis (includes generic budget terms)\nFM_STOPWORDS = STOPWORDS | {\n    # Additional generic terms\n    'country', 'countries', 'people', 'need', 'needs', 'work', 'working', 'works',\n    'world', 'global', 'national', 'local', 'companies', 'company', 'business',\n    'cost', 'costs', 'price', 'prices', 'value', 'values', 'market', 'markets',\n    'service', 'services', 'social', 'economic', 'political', 'development',\n    'developing', 'developed', 'progress', 'future', 'current', 'present',\n    'potential', 'opportunity', 'opportunities', 'challenge', 'challenges',\n    'effort', 'efforts', 'action', 'actions', 'step', 'steps', 'approach',\n    'strategy', 'strategies', 'target', 'targets', 'goal', 'goals', 'objective',\n    'objectives', 'priority', 'priorities', 'focus', 'support', 'supporting',\n    'supported', 'assist', 'assistance', 'help', 'helping', 'helps',\n    'achieve', 'achieved', 'achieving', 'success', 'successful', 'effective',\n    'efficiency', 'efficient', 'quality', 'performance', 'standards', 'standard',\n    'capacity', 'capabilities', 'capability', 'resources', 'resource', 'asset',\n    'assets', 'investment', 'investments', 'contribution', 'contributions',\n    'benefit', 'benefits', 'benefiting', 'impact', 'impacts', 'outcomes', 'outcome',\n    'framework', 'frameworks', 'structure', 'structures', 'process', 'processes',\n    'activities', 'activity', 'operations', 'operation', 'management', 'managing',\n    'enterprise', 'enterprises', 'organization', 'organizations', 'institution',\n    'institutions', 'agency', 'agencies', 'department', 'departments', 'ministry',\n    'board', 'boards', 'council', 'councils', 'committee', 'committees',\n    'legislation', 'laws', 'law', 'regulations', 'regulation', 'rules', 'rule',\n    'conditions', 'condition', 'circumstances', 'situation', 'situations',\n    'environment', 'environments', 'context', 'aspects', 'aspect', 'factors',\n    'factor', 'elements', 'element', 'features', 'feature', 'characteristics',\n    'property', 'properties', 'nature', 'forms', 'modes', 'model', 'models',\n}\n\ndef get_distinctive_words(fm, freq_df, n=10, min_freq=2.0, min_len=5):\n    \"\"\"\n    Find words distinctively used by this FM compared to others.\n    Uses ratio: FM_freq / (max_other_FM_freq + 0.5)\n    \"\"\"\n    fm_freq = freq_df[fm]\n    other_fms = [f for f in freq_df.columns if f != fm]\n    other_max = freq_df[other_fms].max(axis=1)\n    \n    # Calculate distinctiveness ratio\n    ratio = fm_freq / (other_max + 0.5)\n    \n    # Filter criteria\n    valid = (\n        (fm_freq >= min_freq) &                          # Minimum frequency\n        (~fm_freq.index.isin(FM_STOPWORDS)) &           # Not a stopword\n        (fm_freq.index.str.len() >= min_len) &          # Minimum length\n        (fm_freq.index.str.islower()) &                  # Lowercase only (filter acronyms)\n        (fm_freq.index.str.isalpha())                    # Alphabetic only\n    )\n    \n    return ratio[valid].nlargest(n)\n\n# Calculate distinctive words for each FM\nprint(\"=\" * 70)\nprint(\"FINANCE MINISTER DISTINCTIVE WORDS\")\nprint(\"=\" * 70)\nprint(\"(Words each FM used significantly more than other ministers)\")\nprint()\n\nfm_distinctive_words = {}\n\nfor fm in sorted(fm_freq_df.columns, key=lambda x: min([extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == x])):\n    distinctive = get_distinctive_words(fm, fm_freq_df, n=10)\n    fm_distinctive_words[fm] = distinctive\n    \n    years = [extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == fm]\n    era = f\"{min(years)}-{max(years)}\" if min(years) != max(years) else str(min(years))\n    \n    print(f\"\\n{fm} ({era}):\")\n    print(\"-\" * 50)\n    for word, ratio in distinctive.items():\n        freq = fm_freq_df.loc[word, fm]\n        print(f\"  {word:<20} (freq: {freq:.1f}/10k, ratio: {ratio:.1f}x)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create summary table of FM pet words (top 3 per FM)\nprint(\"\\n\" + \"=\" * 70)\nprint(\"SUMMARY: FINANCE MINISTER PET WORDS (Top 3)\")\nprint(\"=\" * 70)\nprint(\"\\nFor curated_story.json fm_words field:\\n\")\n\nfm_summary = []\nfor fm in sorted(fm_freq_df.columns, key=lambda x: min([extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == x])):\n    years = [extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == fm]\n    decade = f\"{min(years)//10*10}s\" if min(years)//10 == max(years)//10 else f\"{min(years)//10*10}s-{max(years)//10*10}s\"\n    \n    top_words = list(fm_distinctive_words[fm].head(3).index)\n    \n    fm_summary.append({\n        'fm': fm,\n        'era': decade,\n        'years': f\"{min(years)}-{max(years)}\",\n        'words': ', '.join(top_words)\n    })\n    \n    print(f'{{\"fm\": \"{fm}\", \"era\": \"{decade}\", \"words\": \"{\", \".join(top_words)}\"}}')\n\n# Create DataFrame for display\nsummary_table = pd.DataFrame(fm_summary)\nprint(\"\\n\")\nprint(summary_table.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export FM distinctive words to JSON\nfm_export_data = {\n    'methodology': {\n        'metric': 'distinctiveness ratio = FM_freq / (max_other_FM_freq + 0.5)',\n        'min_frequency': 2.0,\n        'min_word_length': 5,\n        'normalized_to': 'mentions per 10,000 words'\n    },\n    'finance_ministers': []\n}\n\nfor fm in sorted(fm_freq_df.columns, key=lambda x: min([extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == x])):\n    years = [extract_year(f.name) for f in CORPUS_PATH.glob('*.txt') if extract_fm(f.name) == fm]\n    \n    distinctive = fm_distinctive_words[fm]\n    words_data = [\n        {\n            'word': word,\n            'frequency': round(fm_freq_df.loc[word, fm], 2),\n            'distinctiveness_ratio': round(ratio, 2)\n        }\n        for word, ratio in distinctive.items()\n    ]\n    \n    fm_export_data['finance_ministers'].append({\n        'name': fm,\n        'year_range': [min(years), max(years)],\n        'num_speeches': fm_file_count[fm],\n        'total_words': total_words_by_fm[fm],\n        'distinctive_words': words_data\n    })\n\nwith open('fm_distinctive_words.json', 'w') as f:\n    json.dump(fm_export_data, f, indent=2)\n\nprint(\"FM distinctive words exported to fm_distinctive_words.json\")\nprint(f\"\\nSample entry:\")\nprint(json.dumps(fm_export_data['finance_ministers'][0], indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Persistent Words Analysis (Across All Decades)\n\nSome words appear consistently across all six decades of budget speeches. These represent the enduring expectations of the social compact — demands that never went away.\n\n### Methodology\n\n1. Group speeches by decade (1960s, 1970s, ..., 2020s)\n2. Calculate normalized frequency for each word per decade (mentions per 10,000 words)\n3. Identify words that:\n   - Appear in **every decade** with frequency ≥ 1.0 per 10k words\n   - Are not stopwords\n   - Show meaningful presence across the corpus",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Group speeches by decade\ndef get_decade(year):\n    \"\"\"Convert year to decade label (e.g., 1965 -> '60s', 2024 -> '20s')\"\"\"\n    decade_num = (year // 10) % 10\n    return f\"{decade_num}0s\"\n\nspeeches_by_decade = defaultdict(list)\n\nfor year, texts in speeches_by_year.items():\n    decade = get_decade(year)\n    speeches_by_decade[decade].extend(texts)\n\n# Sort decades in chronological order\ndecade_order = ['60s', '70s', '80s', '90s', '00s', '10s', '20s']\ndecades_present = [d for d in decade_order if d in speeches_by_decade]\n\nprint(\"Speeches grouped by decade:\")\nprint(\"-\" * 40)\nfor decade in decades_present:\n    combined = ' '.join(speeches_by_decade[decade])\n    words = tokenize(combined)\n    print(f\"{decade}: {len([y for y in speeches_by_year.keys() if get_decade(y) == decade])} years, {len(words):,} total words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate word frequencies per decade\nword_counts_by_decade = {}\ntotal_words_by_decade = {}\n\nfor decade in decades_present:\n    combined = ' '.join(speeches_by_decade[decade])\n    words = tokenize(combined)\n    word_counts_by_decade[decade] = Counter(words)\n    total_words_by_decade[decade] = len(words)\n\n# Get all unique words across decades\nall_words_decades = set()\nfor counter in word_counts_by_decade.values():\n    all_words_decades.update(counter.keys())\n\n# Build normalized frequency matrix (words x decades)\ndecade_freq_data = {}\nfor decade in decades_present:\n    total = total_words_by_decade[decade]\n    counter = word_counts_by_decade[decade]\n    decade_freq_data[decade] = {word: (counter.get(word, 0) / total) * 10000 for word in all_words_decades}\n\ndecade_freq_df = pd.DataFrame(decade_freq_data)\ndecade_freq_df = decade_freq_df[decades_present]  # Ensure correct column order\ndecade_freq_df = decade_freq_df.fillna(0)\n\nprint(f\"Decade frequency matrix shape: {decade_freq_df.shape} (words x decades)\")\ndecade_freq_df.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Find words that appear consistently across ALL decades\n# Criteria: \n# - Present in every decade with frequency >= MIN_THRESHOLD\n# - Not a stopword\n# - Related to expectations/demands (effort, skills, competitiveness, etc.)\n\nMIN_DECADE_FREQ = 1.0  # Minimum frequency per decade (per 10k words)\n\n# Check which words appear in all decades above threshold\nall_decades_present = decade_freq_df.min(axis=1) >= MIN_DECADE_FREQ\n\n# Filter out stopwords\npersistent_stopwords = STOPWORDS | FM_STOPWORDS | {\n    'workers', 'businesses', 'families', 'seniors',  # These are rising words\n    'labor', 'workforce', 'employment', 'employed', 'employees',\n    'training', 'productivity', 'productive', 'production'\n}\nnot_stopword = ~decade_freq_df.index.isin(persistent_stopwords)\n\n# Apply filters\nconsistent_words = decade_freq_df[all_decades_present & not_stopword]\n\nprint(f\"Words appearing in ALL {len(decades_present)} decades (≥{MIN_DECADE_FREQ}/10k each):\")\nprint(f\"Found {len(consistent_words)} consistent words\")\nprint()\n\n# Calculate overall presence score (average across decades)\nconsistent_words['avg_freq'] = consistent_words[decades_present].mean(axis=1)\nconsistent_words['min_freq'] = consistent_words[decades_present].min(axis=1)\nconsistent_words['max_freq'] = consistent_words[decades_present].max(axis=1)\nconsistent_words['consistency'] = consistent_words['min_freq'] / consistent_words['max_freq']  # 1.0 = perfectly consistent\n\n# Sort by average frequency\nconsistent_sorted = consistent_words.sort_values('avg_freq', ascending=False)\n\nprint(\"Top 30 consistent words (by average frequency):\")\nprint(\"-\" * 80)\nprint(f\"{'Word':<20} {'Avg':>8} {'Min':>8} {'Max':>8} {'Consist.':>8}\")\nprint(\"-\" * 80)\nfor word in consistent_sorted.head(30).index:\n    row = consistent_sorted.loc[word]\n    print(f\"{word:<20} {row['avg_freq']:>8.1f} {row['min_freq']:>8.1f} {row['max_freq']:>8.1f} {row['consistency']:>8.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Get rates for specific \"demand\" words that persist across decades\n# These represent enduring expectations: effort, skills, competitiveness\n\n# Target words related to expectations/demands\ndemand_related_words = [\n    'effort', 'efforts',\n    'skill', 'skills', 'skilled',\n    'train', 'trained',\n    'competitive', 'competitiveness', 'compete',\n    'improve', 'improvement', 'improvements',\n    'contribute', 'contribution', 'contributions',\n    'upgrade', 'upgrading',\n    'adapt', 'adaptable',\n    'productive', 'productivity',\n    'efficient', 'efficiency',\n    'innovative', 'innovate',\n    'strive', 'striving'\n]\n\nprint(\"=\" * 80)\nprint(\"PERSISTENT DEMAND WORDS - Frequency by Decade\")\nprint(\"=\" * 80)\nprint(\"(Mentions per 10,000 words)\")\nprint()\nprint(f\"{'Word':<18}\", end='')\nfor d in decades_present:\n    print(f\"{d:>8}\", end='')\nprint(f\"{'Avg':>8}\")\nprint(\"-\" * 80)\n\nfor word in demand_related_words:\n    if word in decade_freq_df.index:\n        row = decade_freq_df.loc[word, decades_present]\n        avg = row.mean()\n        min_val = row.min()\n        # Only show if present in at least 5 decades\n        if (row >= 0.5).sum() >= 5:\n            print(f\"{word:<18}\", end='')\n            for d in decades_present:\n                val = row[d]\n                print(f\"{val:>8.1f}\", end='')\n            print(f\"{avg:>8.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export persistent words for curated_story.json\n# Select meaningful words that represent enduring expectations/demands\n\nSELECTED_PERSISTENT_WORDS = ['effort', 'skills', 'training', 'competitive', 'improve', 'productivity']\n\nprint(\"=\" * 70)\nprint(\"PERSISTENT WORDS FOR curated_story.json\")\nprint(\"=\" * 70)\nprint(\"\\nword_data format for 'The expectation persisted' section:\\n\")\n\nword_data_export = []\n\nfor word in SELECTED_PERSISTENT_WORDS:\n    if word in decade_freq_df.index:\n        rates = [round(decade_freq_df.loc[word, d], 1) for d in decades_present]\n        word_entry = {\"word\": word, \"rates\": rates}\n        word_data_export.append(word_entry)\n        print(f'{{\"word\": \"{word}\", \"rates\": {rates}}}')\n\nprint(f'\\n\"decades\": {decades_present}')\n\n# Also export to JSON file\npersistent_export = {\n    'description': 'Words that appeared consistently across all decades of budget speeches',\n    'metric': 'mentions per 10,000 words',\n    'decades': decades_present,\n    'word_data': word_data_export\n}\n\nwith open('persistent_words_data.json', 'w') as f:\n    json.dump(persistent_export, f, indent=2)\n\nprint(\"\\n\\nExported to persistent_words_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}